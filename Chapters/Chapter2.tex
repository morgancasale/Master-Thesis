% Chapter Template

\chapter{Background} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{PCB coils}
\label{Chapter2/PerspProj}
Perspective projection is a fundamental concept in computer graphics and computer vision. It's a mathematical technique used to simulate how 3D scene or object appears when projected onto a 2D surface, such as a computer scene or image plane. The goal of perspective projection is to create a realistic representation of how objects in the 3D world would look from a particular viewpoint, taking into account the effects of distance and perspective. Some fundamental principles of perspective projections are:
\begin{itemize}
    \item\textbf{Vanishing Point:} objects that are far away from the camera appear smaller and converge to a single point in the distance, called vanishing point. This effect creates a sense of depth and realism in the projected image.
    \item\textbf{Depth Perception:} Perspective projection accurately portrays the relative depth of objects, making objects closer to the camera larger and objects father smaller. This mimics the way human eye and camera lens perceive depth in the real world.
    \item\textbf{Foreshortening:} Perspective Projection results in foreshortening, where objects viewed from an angle are distorted in their shape and dimensions. This distortion is crucial for creating realistic images.
    \item\textbf{Depth Cues:} Perspective Projection includes depth cues, such as the overlap of objects, changes in size, and relative position of objects in the field of view., which help the viewer understand the spacial relationships between objects.
\end{itemize}

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.8]{Figures/Chapter2/perspective.jpg}
    \caption[Perspective projection]{Perspective projection.}
    \label{fig:Projection}
\end{figure}

In computer graphics, the perspective matrix (or projection matrix) is used to transform 3D points in 2D coordinates on the screen. This projection is a crucial step in rendering 3D scenes in 2D images.\parencite{HughesDamEtAl13}\\
\noindent
The perspective matrix has several configurations depending on the specific conventions, camera parameters or coordinate systems used. The mono camera used in this project is considered ideal, with negligible distortion coefficient and squared field of view. \\
The field of view (FOV) is a fundamental concept in optics, computer graphics and computer vision. It refers to the extent of the observable world that can be seen through a particular device, such as camera, human eye or computer screen. The FOV determines the angle within which object or scenes are visible and it's usually specified as an angular quantity. In this setup, a squared field of view (FOV) implies that the vertical viewing angle is the same as the horizontal viewing angle. \\
The image below present a visual representation of the FOV:

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.3]{Figures/Chapter2/FOV.png}
    \caption[FOV]{Visual FOV representation.}
    \label{fig:FOV}
\end{figure}

\noindent
As discussed above, a simple implementation of the perspective matrix is enough for our purposes. The matrix is defined as follows:

\begin{equation}
    \textbf{P} = 
    \begin{bmatrix}
    \frac{f}{a_{r}} & 0 & 0 & 0\\
    0 & f & 0 & 0\\
    0 & 0 & \frac{(z_{far} + z_{near})}{(z_{near} - z_{far})} & \frac{2*z_{far}*z_{near}}{(z_{near} - z_{far})}\\
    0 & 0 & -1 & 0
\end{bmatrix}
\end{equation}

The \textit{f} is the focal length of the camera and it's computed as $f = 1/tan(\frac{FOV}{2})$ with \textit{FOV} expressed in radians. The $a_{r}$ is the aspect ratio coefficient needed if the horizontal field of view is different from the vertical one, in this configuration it's 1. The $z_{near}$ and $z_{far}$ represent the distances to the near and far clipping planes, respectively. They define the range of distances of objects in the scene when projected from 3D space to 2D space.

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.58]{Figures/Chapter2/projection.png}
    \caption[Perspective projection of an object]{Object's perspective projection}
    \label{fig:Opp}
\end{figure}

% -- Subsection 1.1
\subsection{Overview of magnetic field production with coils}

% -- Subsection 1.2
\subsection{Planar coils}

% -- Subsection 1.3
\subsection{Multi-layer PCB coils}

% -- Subsection 1.4
\subsection{Challenges of miniaturization}


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Flexible PCBs}
The monocular camera model, a foundational concept in computer vision and imaging, mimics the behavior of a pinhole camera to represent the process of capturing and projecting images. This model simplifies the intricate workings of an optical system to enable a comprehensive understanding of the relationship between the three-dimensional (3D) world and the resulting two-dimensional (2D) image.

In this model, we envision light passing through a minute aperture, analogous to a pinhole, projecting an inverted image onto a photosensitive surface, often a digital or analog image sensor. Key intrinsic parameters, including the focal length and principal point, characterize the camera's optical properties. The focal length dictates the scale of the captured scene, while the principal point marks the location where the optical axis intersects the image plane.

Coordinate systems play a pivotal role in this model, with the camera coordinate system centered at the optical center and the image coordinate system representing points on the image plane. The intrinsic parameters, encompassing the focal length and principal point, along with extrinsic parameters like rotation and translation, define the camera's pose in space.

A fundamental mathematical concept, perspective projection (described in depth in section \textbf{\ref{Chapter2/PerspProj}}), captures the transformation from 3D points in the camera coordinate system to their 2D projections on the image plane. This equation involves the intrinsic matrix, encapsulating the focal lengths and principal points, and the coordinates of the 3D points.

While the monocular camera model serves as a fundamental abstraction, it often considers ideal conditions, neglecting real-world imperfections such as lens distortion. Distortion correction parameters may be introduced to refine the model, enhancing its accuracy.

Monocular cameras find extensive applications in various domains, from smartphones to surveillance cameras. They are integral to computer vision tasks, such as object recognition, pose estimation and structure-from-motion. The simplicity and versatility of the monocular camera model make it a cornerstone for comprehending the principles of image formation and interpretation in the broader field of computer vision.
\newpage
The camera model displacement and orientation with respect to the world's reference system can be expressed as follow:

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.3]{Figures/Chapter2/Camera_model.jpg}
    \caption[Camera Model]{Camera Model}
    \label{fig:CameraModel}
\end{figure}

\begin{enumerate}
    \item displacement $\textbf{w}_0$ of the origin of the camera reference system.
    \item Pan of x axis (rotation around x axis).
    \item Tilt of z axis (rotation around z axis).
    \item Displacement $\textbf{r}$ of the image plane with respect to the center of the joint, on which the camera is mounted and around which can be rotated.
\end{enumerate}

For the two linear displacements, their respective translation matrices are obtained as follow:
\begin{equation}
    \textbf{G} = 
    \begin{bmatrix}
    \mathbbm{1} & -\textbf{w}_0\\
    0 & 1\\
    \end{bmatrix}
    \quad 
    \textbf{C} = 
    \begin{bmatrix}
    \mathbbm{1} & -\textbf{r}\\
    0 & 1\\
    \end{bmatrix}
\end{equation}

The resultant rotation matrix, given the above rotations about x and z axis is:
\begin{equation}
    \textbf{R} = R_\alpha R_\theta = 
    \begin{bmatrix}
    cos(\theta) & sin(\theta) & 0 & 0\\
    -sin(\theta)cos(\alpha) & cos(\theta)cos(\alpha) & sin(\alpha) & 0\\
    sin(\theta)sin(\alpha) & -cos(\theta)sin(\alpha) & cos(\alpha) & 0\\
    0 & 0 & 0 & 1
    \end{bmatrix}
\end{equation}

To compute the total transformation of a point from the 3D world to the image plane a perspective projection matrix $\textbf{P}$ is needed. The next section is dedicated to the deep description of this concept.

The total transformation is computed as follows:
\begin{equation}
    c_h = \textbf{PCRG}w_h
\end{equation}

% -- Subsection 2.1
\subsection{Pro and Cons of flexible coils with respect to normal ones}

% -- Subsection 2.2
\subsection{Non-standard applications of PCBs}


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Application challenges of Flexible PCB coils}
Object pose estimation is a fundamental task in computer vision that involves determining the spatial orientation and position of an object in a given environment. The "pose" refers to the object's six degrees of freedom (6DOF), which include its translation (movement) along the x, y, and z axes and its rotation around these axes (roll, pitch, and yaw). The goal is to accurately understand how an object is positioned and oriented with respect of a reference coordinate system.

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{Figures/Chapter2/Yaw_Axis_Corrected.png}
    \caption[Roll, Pitch and Yaw rotations (RPY) of an object.]{Roll, Pitch and Yaw rotations (RPY) of an object.}
    \label{fig:RPY}
\end{figure}

Key aspects of object pose estimation include:
\begin{itemize}
    \item \textbf{Object Representation:} Objects are often represented by 3D geometric models or point clouds. These models describe the shape and structure of the object in a coordinate system.
    \item \textbf{Sensors:} Pose estimation relies on data acquired from sensors, such as cameras or depth sensors. Each sensor type has its strengths and limitations in capturing the necessary information for pose estimation.
    \item \textbf{Feature Extraction:} Features or keypoints (landmarks) are identifiable points on the object's surface that can be matched between the 3D model and the sensor data. These features serve as reference points for determining pose.
    \item \textbf{Matching and Correspondence:} The process involves finding correspondences between the features in the 3D model and those detected in the sensor data. Techniques such as feature matching and point cloud registration are employed for this purpose.
    \item \textbf{Pose Computation:} Once correspondences are established, algorithms calculate the pose parameters. This step involves identifying translation and rotation that best align the 3D model with the observed features in the sensor data.
    \item \textbf{Optimization:} Iterative optimization methods are used to refine the initial pose estimation. These methods aim to minimize the difference between predicted and observed feature locations.
    \item \textbf{Applications:} Object pose estimation is crucial in various applications, such as robotic manipulation, augmented reality, autonomous navigation, and quality control in manufacturing. It enables machines to interact with the environment and make informed decisions based on the perceived spatial relationships of objects.
\end{itemize}

\begin{figure}[th]
    \centering
    \includegraphics[scale=1]{Figures/Chapter2/Pose_estimation.png}
    \caption[Example of applications of pose estimation.]{Example of applications of pose estimation.}
    \label{fig:PoseEstimation}
\end{figure}

Accurate object pose estimation is essential for tasks where knowing an object's precise location and orientation is critical for effective and safe interaction with the environment. Advances in computer vision, machine learning, and sensor technologies contribute to the ongoing improvement of object pose estimation methods.


% -- Subsection 3.1
\subsection{Rise of high resistance}

% -- Subsection 3.2
\subsection{Parasitic effects due to AC current}

% -- Subsection 3.3
\subsection{Joule effect}

\subsubsection{Need for heat dissipation}

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------
\newpage
\section{Running PCB coils}\footnote{The \textit{"T81-558: Applications of Deep Neural Networks"} course of the Washington University \cite{T81-558} and the \textit{"CS231n: Deep Learning for Computer Vision"}\cite{CS231n} one are used as reference for the presentation of the topic.}
In the vast realm of artificial intelligence, a crucial discipline emerges: machine learning (ML), a transformative approach to programming that defies conventional code-based paradigms. At its essence, machine learning empowers systems to evolve and improve through experiences, learning patterns from data rather than relying on explicit instructions. This paradigm shift opens the door to a new era of problem-solving, where algorithms become adept at making decisions, predictions, and inferences based on the information they ingest.

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.4]{Figures/Chapter2/Timeline-of-AI-to-Deep-Learning.png}
    \caption[Timeline of AI to Deep Learning.]{Timeline of AI toward Deep Learning.}
    \label{fig:TimelineAI}
\end{figure}

At the core of machine learning lies the intricate paradigm between algorithms and data. The learning process begins with a robust dataset, a collection of input features paired with corresponding outcomes. This data serves as the fodder for ML algorithms, mathematical constructs that decode patterns and relationships within the information. As the algorithm processes the data, it continually adjusts its internal parameters to better align its predictions with the ground truth.

Machine learning encompasses various learning paradigms, each tailored to specific challenges. In supervised learning, algorithms learn from labeled data, associating inputs with desired outputs. Unsupervised learning tackles unlabeled data, seeking inherent structures and relationships. Reinforcement learning introduces the element of interaction, where algorithms learn through trial and error, navigating an environment and adapting based on feedback.

As the capabilities of machine learning burgeoned, a more specialized branch emerged: deep learning. This paradigm shift brought about the rise of deep neural networks, inspired by the complexity of the human brain. Deep learning algorithms, structured as multi-layered neural networks, exhibit an unparalleled ability to automatically learn hierarchical representations from data.

Within the deep learning framework, neural networks delve into the intricacies of feature learning. These models, often identified as deep neural networks, boast multiple layers that autonomously extract increasingly complex features. The depth of these networks empowers them to uncover intricate patterns, making them particularly effective in tasks such as image and speech recognition.

The training process in machine learning and deep learning involves an iterative refinement of models. Algorithms, armed with backpropagation mechanisms, fine-tune their internal parameters to minimize the discrepancy between predicted and actual outcomes. This continual optimization results in models that not only perform well on training data but also generalize effectively to new, unseen data.

The applications of machine learning and deep learning span a multitude of domains, reshaping the landscape of technology and problem-solving. From image and speech recognition to natural language processing and autonomous systems, these methodologies have demonstrated unprecedented success. As we delve into the intricacies of machine learning and explore the depths of deep learning, we uncover the transformative power of algorithms that learn, adapt, and redefine the boundaries of artificial intelligence.
% -- Subsection 4.1
\subsection{High current needs}
The neural network is one of the first deep learning model. It emulates how neurons function in the human brain using connected circuits to simulate the intelligent behaviour.\\
Neural networks accept as input a feature vector with fixed length and produces as output a vector of predicted values with fixed length as well. Usually changing the input or output vector length means redesigning the entire structure.\\
The term \textit{"vector"} is usually referred to 1D arrays but with modern neural network it is increasingly common to find multiple dimensions arrays (as I will discuss later with the CNN).\\
The term \textit{"dimension"} can be misleading in neural networks, since, used in sense of the dimension of a vector, it refers to the number of elements present in that vector (for instance, a neural network with ten input neurons has ten dimensions). However, in the case of CNN the input has multiple dimensions, so 2D input to a neural network with 128x128 pixels leads to a configuration of 16,368 input neurons. In the first example the neural network will be defined as 2D NN, in the second one 16,368D.

\begin{figure}[th]
    \centering
    \includegraphics{Figures/Chapter2/NeuralNetwork.jpg}
    \caption[Neural Network]{Neural Network's layer representation.}
    \label{fig:NeuralNetwork}
\end{figure}

% -- Subsection 4.2
\subsection{Costant voltage vs current supply}
The neural network can functions in regression or classification. In the first case the output is a number predicted on the base of the input data, in the second one the identification of a specific class o category. It is important to note that the output of a regression or two-class classification model is a number (binary for the classification 1 for true value 0 or -1 for false value), but in case of multi-class classification the neural network has an output neuron for each category. 

% -- Subsection 4.3
\subsection{Difference between DC and AC signals}
The artificial neuron receives as input one or more sources from input data or previous layer neurons. It multiplies each of those inputs by a respective weight and sums these multiplications together (sometimes also a bias factor is added). The result is passed to an activation function that determines the output of the neuron.\\
\begin{equation}
    f(x,w) = \phi(\sum_i(w_i\*x_i))
\end{equation}

In the above equation the variables \textit{x} and \textit{w} represent the input and the weights of the neuron respectively, the \(\phi(\cdot)\) the activation function and \textit{f(x,w)} the output of the neuron.

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.6]{Figures/Chapter2/Neuron.jpg}
    \caption[Neuron]{Artificial Neuron representation.}
    \label{fig:Neuron}
\end{figure}

The neurons can be classified in four categories, depending on their position in the architecture:
\begin{itemize}
    \item\textbf{Input Neurons:} each input neuron is mapped to an element in the feature vector.
    \item\textbf{Hidden Neurons:} intermediate neurons responsible of the abstraction of the neural networks to process the input into the output.
    \item\textbf{Output Neurons:} each output neuron calculates one part of the output.
    \item\textbf{Bias Neurons:} introduces an additional learnable parameter that improves the prediction's accuracy by shifting the activation threshold on the activation function.
\end{itemize}

%\subsubsection{Activation Functions}
Activation functions, or transfer functions, have the role of computing the output of each layer of a neural network. Historically the more common are hyperbolic tangent, sigmoid, or linear activation functions. However, with modern deep learning models, specialized activation functions have been introduced to suit specific applications and tasks:
\begin{itemize}
    \item\textbf{Rectified Linear Unit (ReLU):} use for the output hidden layers.
    \item\textbf{Softmax:} used for the output of classification neural networks.
    \item\textbf{Linear:} used for the output of regression neural networks.
\end{itemize}
In particular ReLU has gained rapid popularity in deep learning due to its ability to yield superior training results. Before the era of deep learning, the sigmoid function was the most prevalent activation function. However, as modern frameworks like PyTorch frequently train neural network using gradient descent optimization, computing partial derivatives of the sigmoid became a computationally challenging operation. The introduction of the Rectified Linear Unit significantly simplified this computation leading to improved performance and faster training.

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.4]{Figures/Chapter2/ActivationFunctions.png}
    \caption[Sigmoid vs. ReLU]{Sigmoid and ReLU activation functions graphic representation.}
    \label{fig:ActivationFunctions}
\end{figure}

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Convolutional Neural Network (CNN)}
The Convolutional Neural Network (CNN) is a neural network technology that has deeply impacted the area of computer vision. The original concept of a CNN was introduced by Fukushima in 1980\parencite{fukushima} and subsequent significant advancements were made by LeCun \emph{et al.}\parencite{Lecun}.

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.4]{Figures/Chapter2/CNN.png}
    \caption[Convolution Neural Network]{Convolutional Neural Network visual representation.}

    \label{fig:CNN}
\end{figure}
The CNN, in contrast with most neural networks, has a specific order of the input elements and it is crucial to the training. The inputs are arranged into a grid which represents the input image, the highest-level unit. On the other hand each pixel is the smallest unit within the image and represents a scalar value denoting the intensity or color at a specific location. The images can be colored (expressed on three channels RGB) or grayscale with a single channel.
Due to the amount of data, additional layers are needed to lighten the computation load. The layers works together to extract features from the input data and predict the output. The additional layers are:
\begin{itemize}
    \item\textbf{Convolutional Layers:} apply filters or kernels to perform convolution operations, extracting local features like edges and shapes.
    \item\textbf{Pooling Layers:} reduce the spacial dimensions of feature maps, making the network more robust to scale and orientation variation.
    \item\textbf{Dense Layers:} these layers connect all neurons to every neuron in the previous and subsequent layers, performing high-level feature extraction and decision-making.
    \item\textbf{Flattening Layer:} reshape the output from convolutional layers to 1D vector, allowing it to connect to fully connected (dense) layers.
\end{itemize}

%\subsubsection{Training and Testing Processes}
The training process begins with the initialization of the network's weight and biases; usually these parameters are set to small random values.\\
The input is then fed through the network, the data propagates through each layer and predictions are produced. A loss function is used to quantify the error between the predicted and the ground truth values. The choice of the loss function depends on the specific task, the most common are MSE for regression and cross-entropy for classification. The backpropagation algorithm is employed to compute the gradients of the loss with respect to the network's parameters (weights and biases).\\
This process is essentially the chain rule from calculus, which calculates how small changes in each parameter affect the loss. Using the gradients computed during backpropagation, the network's weights and biases are updated through an optimization algorithm.

The goal is to adjust the parameters in a way that minimizes the loss function. The process is then iterated over the entire training dataset with a process called epoch. Multiple epochs are typically performed to improve the model's performance.

Periodically, the model's performance is evaluated on a separated validation dataset that the network has not seen during training. This method helps monitor the model's generalization capabilities.

After training and validation, the model is tested on a completely unseen test dataset to evaluate its performance on new, independent data.



%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------
\section{Coil-Magnet-Membrane system}\footnote{The \textit{"T81-558: Applications of Deep Neural Networks"} course of the Washington University \cite{T81-558} and the \textit{"CS231n: Deep Learning for Computer Vision"}\cite{CS231n} one are used as reference for the presentation of the topic.}

% -- Subsection 5.1
\subsection{Neodynium magnets (magnetic strenght wrt class and dimensions)}

% -- Subsection 5.2
\subsection{Magnetic "coupling" between magnet and coil}

\subsubsection{Relation between coil and magnet diameter}

\subsubsection{Distance between coil and magnet}

% -- Subsection 5.3
\subsection{Membrane-Magnetsystem}

%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------
\section{Membrane-Finger haptic system }
