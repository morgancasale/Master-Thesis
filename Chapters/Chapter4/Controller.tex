\section{Controller}
Figure \textbf{\ref{fig:Offline Pipeline}} describes the overall offline pipeline of the implemented methodology, which consists of several main modules.\\
From the satellite CAD model nine landmarks have been manually selected (details about the selection criteria explained in section \textbf{\ref{Chapter4/LandmarksSel}}). The 2D-3D correspondence of each landmark is used for training the \textit{Landmark regression} module to predict the nine landmarks image position and the \textit{Landmark Mapping} module to reconstruct the 3D position of each landmark from their image position.
The implemented code can be accessed in \cite{JMF}.

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.55]{Figures/Chapter4/Offline Pipeline.png}
    \caption[Offline pipeline]{Offline pipeline of the implemented pose estimator.}
    \label{fig:Offline Pipeline}
\end{figure}

% -- Subsection 4.1
\subsection{ESP32 waveform generator for a single module}

\subsubsection{Controller code}

\subsubsection{Esp32 DAC limitations}

\subsubsection{Esp32 DAC Noise}
\label{Chapter4/2D-3D}
The computation of the 3D position of landmarks in the training images involves a transformation process that leverages the relative positions of the selected landmarks with respect to the satellite's reference system, as well as the position and orientation of the camera.

\begin{figure}[th]
    \centering
    \includegraphics[scale=1]{Figures/Chapter4/Satellite_CAD_Model.png}
    \caption[CAD Model]{Selected Landmarks on the CAD Model.}
    \label{fig:CAD Model}
\end{figure}

The 3D landmarks are defined in the context of the satellite's reference system. These landmarks represent specific features on the satellite's structure. Their 3D coordinates are known relative to the satellite's own coordinate system thanks to the CAD model.

The camera that captures the training images is mounted on the wrist of a robotic arm on the servicer satellite and its position and orientation with respect to the satellite's reference system is paired with images in the training dataset.

To determine the 3D position of the landmarks in the camera's coordinate system, a simple operation is performed, transforming the landmarks' positions from the satellite's frame to the camera's frame by means of the systems' transformation matrix.
\begin{equation}
    x_{cam,i} = 
\begin{bmatrix}
    \textbf{R}^\textbf{T} & t_{c,s}\\
    0 & 1
\end{bmatrix}
x_{sat,i} \quad i = 1,...,N \quad N:\textrm{number of landmarks}
\end{equation}
Furthermore, perspective projection transformation (described in section \textbf{\ref{Chapter2/PerspProj}}) is performed, relating 3D world coordinates \{\(x_{i}\)\} to 2D image coordinates \{\(z_{i}\)\} by means of the perspective matrix \{\(\textbf{P}\)\}.
\begin{equation}
    z_{i} = \textbf{P}*x_{cam,i} \quad i = 1,...,N
\end{equation}

% -- Subsection 4.2
\newpage
%\subsection{RasPi controller for multiple modules}
\label{Chapter4/LandReg}
Each training image is first pre-processed (description of the pre-processing process in section \textbf{\ref{Chapter4/Pre-Processing}}) and then coupled with a set of ground truth 2D landmarks \{\(\textbf{z}_{i}\)\}, as described in section \textbf{\ref{Chapter4/2D-3D}}. Those labels are used to supervise the training of the regression model to predict the 2D position of landmarks in the testing images. An additional label is introduced to handle images that capture only partially the satellite, the visibility coefficient \{\(v_{i}\)\}.
\begin{equation}
  v_{i}=\begin{cases}
    1, & \text{if $\textbf{z}_{i}$ is inside image frame}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}
The output of the model is a tensor of 9 heatmaps, one for each landmark $h(\textbf{z}_{i}^p)$. The ground truth heatmaps $h(\textbf{z}_{i})$ are generated as 2D normal distributions with mean equal to the ground truth location of each landmark, and standard deviation of 1-pixel (code in appendix \textbf{\ref{AppendixA1}}).

The model is trained from scratch by minimizing the following customized loss:
\begin{equation}
l = \frac{1}{N}\sum_{i=1}^N v_i(h(\textbf{z}_{i}^p) - h(\textbf{z}_{i}))^2
\end{equation}
The loss function \textit{l} is defined on a single image and in a mini batch \textit{l} is simply averaged. The model is trained in 100 epochs with the \textit{Adam optimizer}\cite{kingma2017adam}.

In order to select the landmark location in the predicted heatmap, an additional selection algorithm is introduced.\\
Each predicted heatmap is normalized over [-1,+1] range, where \(-1\) represents a totally black pixel and +1 totally white. After an accurate study on the heatmaps that presents an accurately predicted landmark, the main result is that most of those images presented maximum value \(>-0.5\), so a first threshold \(t_1\) is introduced and set to that value. In only few cases the prediction of the landmark is more imprecise, due, for instance, to similarities with other features in the image. For those cases a second lower threshold is included \(t_2 = -0.6\).

Firstly pixels in the images with value lower than \(t_2\) are by default set to black \((-1)\) to clear the image from impurities, then, for those heatmaps with maximum value over \(t_1\), the position of their maximum value is taken as landmark position. On the other hand, for those who present a maximum value within the thresholds \(t_1\) and \(t_2\) an additional check on the heatmap variance is performed: only for those with a variance value over \(2e^{-6}\) the landmark is considered within the image and its location is registered as above. For all the other heatmaps that do not follow these conditions, the landmark is considered out of the image frame (2D location set to default value [-1,-1]) and the visibility coefficient \(v_{i}\) is set to zero.

In case of visually similar landmarks, an additional check is introduced. The position of a landmark registered after recognition is compared with the previously registered landmarks. In case it has a position within a certain pixel range near to another landmark, the one with higher maximum value and variance is registered and the other is considered out of frame (code in appendix \textbf{\ref{AppendixA2}}). Figure \textbf{\ref{fig:Landmark Identification Algorithm}} shows the algorithm's identification process.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figures/Chapter4/Landmark Selection.png}
    \caption[Landmark Identification Algorithm.]{Landmark Identification Algorithm.}
    \label{fig:Landmark Identification Algorithm}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figures/Chapter4/Landmark.png}
    \caption[Landmarks identified by the Landmark Regression module.]{Landmarks identified by the Landmark Regression module.}
    \label{fig:Identified landmarks}
\end{figure}



\newpage
%-----------------------------------
%	SUBSECTION 3
%-----------------------------------
% \subsection{Landmark Mapping}
The Landmark Mapping is a neural network designed to estimate the 3D positions of landmarks using their 2D positions, mapping the 2D-3D relation.\\
The network takes as input the 2D positions of landmarks obtained from the Landmark Regression module (\textbf{\ref{Chapter4/LandReg}}) and predicts as output the respective 3D position.

The network has two hidden layers:
\begin{itemize}
    \item The first hidden layer consists of 128 units. It processes the input data and learns complex patterns and relationships between the 2D and 3D coordinates.
    \item The second hidden layer has 64 units and further refines the features learned in the previous layer.
\end{itemize}
ReLU activation functions are applied after the first and second hidden layers. ReLU introduces non-linearity and helps the network capture complex relationships in the data.\\
The output layer is responsible for regressing the 3D positions of the landmarks. Each landmark is represented by a 3D coordinate (x, y, z). The output layer produces these 3D coordinates for all the landmarks.\\
The output of the network is reshaped to organize the predicted 3D coordinates for each landmark. This reshaping ensures that the output is in a format suitable for further processing.\\
The model is trained with the 2D-3D correspondences described in section \textbf{\ref{Chapter4/2D-3D}}, but, before training, the 3D ground truth of landmarks out of image frame (\(v_i = 0\)) is set to default value \([0,0,0]\).\\
The model is trained from scratch by minimizing the following loss:
\begin{equation}
l = \frac{1}{N}\sum_{i=1}^N v_i(\textbf{x}_{i}^p - \textbf{x}_{i})^2
\end{equation}
In the above equation the \(\textbf{x}_{i}^p\) represents the 3D position of the \textit{i}-th landmark predicted by the model, while \(\textbf{x}_{i}\) is its ground truth position.\\
As in the previous model, the loss function \textit{l} is defined on a single group of landmarks and in a mini batch it is simply averaged. The model is trained in a maximum of 150 batches with a \textit{Adam optimizer}\cite{kingma2017adam}.

In order to improve the accuracy of the algorithm, three models are created, each of them specialized in a specific range of distance from the target. The training dataset is so split in three subdatasets depending on the distance on the z axis from the satellite with a 100 images superposition per model from each trajectory. The table below shows the specifications of each model.
\begin{table}[H]
\label{tab:Landmark Mapping Models}
\centering
\begin{tabular}{l|l}
\toprule
Model Name & Covered Range (m)\\
\midrule
M1 & 2.00-1.20 \\
M2 & 1.40-0.50 \\
M3 & 0.70-0.40 \\
\bottomrule
\end{tabular}
\caption{Specifics of three models \textit{M1}, \textit{M2} and \textit{M3}.}
\end{table}
In order to prevent overfitting, an \textit{Early Stopping} algorithm \cite{T81-558} has been introduced. Overfitting occurs when a neural network becomes too specialized in its understanding of the training data, to the point that it struggles to generalize to new, unseen data. The idea behind early stopping is to maintain under control the network's performance, particularly on a separate dataset called validation set, as it undergoes training.
During the training process, the neural network works on refining its parameters using the training dataset. But instead of letting it train tirelessly until convergence, the training process is periodically paused and its performance are evaluated on the validation set. If, over a certain number of consecutive evaluations (determined by a parameter known as "patience"), the network's performance starts to worsen, it's an early warning sign. It suggest that the model is becoming too specialized. Once the point where the validation performance is consistently declining, the training phase is stopped. At this moment, the neural network is considered to have reached its optimal performance on unseen data. Its parameters are taken as the final model, capable to generalize to new unseen data.
