\section{Amplifier circuit}
The online architecture operates in real-time. It processes the input data from the camera and produces as output the satellite pose estimation.

After being pre-processed, the image captured from the camera is passed to the \textit{Landmark regression} module. The latter predicts the 2D location \{\(\textbf{z}_i\)\} of the 9 landmarks along with the visibility coefficient \{\(v_i\)\} for each of them. The \textit{Landmark Mapping} module then uses these data to compute the 3D position of each landmark with respect to the camera frame. The final 6DOF pose of the satellite is then computed from the CPD module. Figure \textbf{\ref{fig:Online Pipeline}} shows the online pipeline of the implemented methodology.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figures/Chapter4/Online Pipeline.png}
    \caption[Online pipeline of the implemented pose estimator.]{Online pipeline of the implemented pose estimator.}
    \label{fig:Online Pipeline}
\end{figure}

% -- Subsection 2.1
\subsection{High Power Operational Amplifiers}
\label{Chapter4/Pre-Processing}
The image captured from the camera is pre-processed in the \textit{Pre-Processing} module. It consists in a bilateral filter, which is a non-linear, edge-preserving smoothing filter that reduces noise while preserving the edges in an image.\\
The mathematical steps behind the bilateral filter involve computing weighted averages of pixel values within a local neighborhood. 
\begin{figure}[th]
    \centering
    \includegraphics[scale=0.32]{Figures/Chapter4/bilateral_filtering.png}
    \caption[Bilateral Filter.]{Bilateral Filter mathematical steps.}
    \label{fig:Bilateral Filter}
\end{figure}

The formula for the bilateral filter operation can be described as follows:

Given an input image \(I(x, y)\) and the filter parameters:
\begin{itemize}
    \item \(d\) (diameter of each pixel's neighborhood)
    \item \(\sigma_c\) (standard deviation of the color space)
    \item \(\sigma_s\) (standard deviation of the spatial space)
\end{itemize}
The filtered output image \(B(x, y)\) can be computed using the following equation for each pixel \((x, y)\):
\begin{equation}
   B(x, y) = \frac{1}{W(x, y)} \sum_{(i, j) \in S} I(i, j) \cdot G_c(I(x, y), I(i, j), \sigma_c) \cdot G_s(x, y, i, j, \sigma_s) 
\end{equation}


Where:
\begin{itemize}
    \item \(S\) is the neighborhood of pixel \((x, y)\), defined by a window of diameter \(d\).
    \item \(G_c\) is the color similarity term, which measures the similarity in color between pixels \((x, y)\) and \((i, j)\) in the color space.
    \item \(G_s\) is the spatial similarity term, which measures the spatial proximity between pixels \((x, y)\) and \((i, j)\).
    \item \(W(x, y)\) is a normalization factor.
\end{itemize}
The \(G_c\) term is defined as a Gaussian function in the color space:
\begin{equation}
    G_c(I(x, y), I(i, j), \sigma_c) = \exp\left(-\frac{\|I(x, y) - I(i, j)\|^2}{2\sigma_c^2}\right)
\end{equation}

The \(G_s\) term is a Gaussian function in the spatial space:
\begin{equation}
    G_s(x, y, i, j, \sigma_s) = \exp\left(-\frac{\|(x, y) - (i, j)\|^2}{2\sigma_s^2}\right)
\end{equation}

\begin{figure}[th]
    \centering
    \includegraphics[scale=0.5]{Figures/Chapter4/Pre-processing.png}
    \caption[Input and pre-processed image.]{On the left the original sample image from the training dataset, on the right the resultant image after the application of the bilaterial filter ($d = 40,\sigma_c = 40,\sigma_s = 200$).}
    \label{fig:Pre-processing}
\end{figure}

The bilateral filter operates by applying these Gaussian weighting functions to compute the weighted average of pixel values within the defined neighborhood, both in color and spatial spaces, resulting in a smoothed image that preserves edges and details. The filter helps reduce noise while keeping the important structures in the image intact.

\subsubsection{Old type of op-amp}
\subsubsection{Power dissipation problems}

% -- Subsection 2.2
\subsection{High Power Voltage Amplifier}
\label{Chapter4/CPD}
As the \textit{Landmark Mapping} module predicts the position of each landmark independently one from the other and so each landmark has its own error over the three axis, the resultant cloud of points is misaligned with respect to the rigid reference position given by the CAD model.\\
In order to estimate the 6DOF pose of the satellite and in the meantime overcome this misalignment problem a mathematical technique is used: Coherent Point Drift.

Two sets of 3D points are present: one is the "target", which represents the 3D position of the landmarks in the camera frame supposing no translation and no rotation, and the other is "source", which is the 3D position of the predicted landmarks.
The "source" point set is only composed by landmarks identified in the image frame (\(v_i = 1\)). It is important to know that the algorithm's performances strongly depend on the number of points in the set so, with the camera approaching the target, some landmarks are cut from the image frame and consequently the pose estimation accuracy reduces.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.23]{Figures/Chapter4/CPD.png}
    \caption[Iterations of the CPD optimization process.]{Iterations of the CPD optimization process.}
    \label{fig:CPD}
\end{figure}